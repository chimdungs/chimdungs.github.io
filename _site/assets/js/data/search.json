[
  
  {
    "title": "Graphics-Ch6. Transformation Matrices",
    "url": "/posts/ch6-Transformation_Matrices/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-07 13:04:00 +0900",
    





    
    "snippet": "Introduction  Transformation matrix는 왜 필요한가 : Rotation, Translation, Scaling, Projection같은 기하학적 변환에서 행렬 곱을 통해 변환되는데 이때 변환행렬이 필요함,",
    "content": "Introduction  Transformation matrix는 왜 필요한가 : Rotation, Translation, Scaling, Projection같은 기하학적 변환에서 행렬 곱을 통해 변환되는데 이때 변환행렬이 필요함,"
  },
  
  {
    "title": "Graphics-Ch4. Ray Tracing",
    "url": "/posts/ch4-Ray-Tracing/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-04 17:34:00 +0900",
    





    
    "snippet": "Rendering  Object based Rendering          각각의 object단위로 고려됨      물체가 존재하는 모든 픽셀들이 찾아지고 업데이트 됨        Image based Rendering          각각의 pixel단위로 고려됨      물체에 영향을 주는 각 pixel들이 찾아지고 업데이트 됨둘의 차이점에 대해...",
    "content": "Rendering  Object based Rendering          각각의 object단위로 고려됨      물체가 존재하는 모든 픽셀들이 찾아지고 업데이트 됨        Image based Rendering          각각의 pixel단위로 고려됨      물체에 영향을 주는 각 pixel들이 찾아지고 업데이트 됨둘의 차이점에 대해서는 ch8에서 더 자세하게 다룬다      Ray Tracing이란  3D Scene을 렌더링하는데에 사용되는 image-order 알고리즘  object-order rendering에서 사용되어지는 수학적 방법1. Basic Ray-Tracing Algorithm  image상의 Pixel에서 ray tracing을 했을 때 보여지는 물체를 찾는 것이 목표  각각의 픽셀은 다른 방향을 “바라본다”          이 보여지는 물체들은 모두 viewing ray 상에 존재한다        카메라에 가장 가깝게 존재하는 viewing ray에 대해 교차하고 있는 특정 Object를 찾는 것이 목표      그 물체가 찾아지면, shading 계산을 통해 intersection point, surface normal, 그리고 다른 정보들을 구해서 pixel color를 결정할 수 있음    Basic Ray Tracer의 세 가지 흐름 :          Ray Generation : Camera geometry에 기반하여, origin과 각 픽셀마다 vieweing ray의 direction을 구하는 것      Ray Intersection : viewing ray와 교차하는 가장 가까운 물체(Object)를 찾는 것      Shading : ray-intersection 과정을 통해 구해진 결과를 기반으로 Pixel color를 계산하는 것        기본 내용을 다룬다, 더 발전된 내용은 chapter 10, 12, 13 등등에 나옴2. Perspective projection  how to mapping 3D objects into 2D image plane?                            Linear perspecrive          straight line =&gt; straight line                                      Parallel projection          projection direction에 따라서 움직여짐                          parallel projection을 통해 보이는 뷰는 orthographic view라고도 부른다          장점 : 변환되어도 size랑 shape은 변화시키지 않고 일정하게 유지한다          단점 : 물체가 view point기준으로 멀어질수록 작게 보인다 =&gt; vanishing point(소실점)과 연관 있음                          this is because eyes and cameras don’t collect light from a single viewing direction(뷰 하나 보이는 것으로 눈과 카메라에서 모든 빛을 수집할 수 없기 때문임)                                           3. Computing Viewing Rays (Ray Generation)\\(p(t) = e + t(s-e)\\)  $e$ : eye(origin point, view point)  $s$ : end point of the ray      $t$ : 시간 축    vector $(s-e)$ 를 view direction로 해석할 수 있다  만일 $t$가 0보다 작다면, view behind에 있다고 해석할 수 있다  “Ray Generation” 단계의 origin과 view direction을 구하는 과정에서 camera frame으로 알려진 orthonormal 좌표계에서 시작해야 한다.          orthonormal coordinate frame : 세 가지 기저 벡터 $u, v, w$으로 구성되어 있음      3.1. Orthographic (Parallel) Views  이미지를 표현하는 4가지 차원 = ${l, r, b, t}$[Orthographic viewing rays 만드는 방법]  ray Starting point(Origin)로 pixel의 image-plane position을 그대로 사용할 수 있다 = $e + u\\mathrm{u} + v\\mathrm{v}$  ray’s Direction으로는 view direction을 가져다가 사용할 수 있다 = $-w$3.2. Perspective Views  각 픽셀마다의 ray들은 같은 origin$(e)$를 공유함          Image Plane이 더이상 $e$ 점에 위치하는 것이 아니라, 거리 $d$만큼 떨어져서 존재함      $d$ : image plane distance 또는 focal length 라고 불린다        view direction은 모두 다름          어떻게 구해지는가? -&gt; image planedml pixel위치와 viewpoint에 의해서 정의된다 = $-dw + u\\mathrm{u} + v\\mathrm{v}$      4. Ray-Object Intersection  앞선 Ray Generation단계를 통해 ray $\\mathrm{e} + t\\mathrm{d}$를 만들었다면, $t &gt; 0$ 인 구간에서 ray와 처음으로 교차되는(만나는) 물체를 찾는 것이 필요하다.  General problem = find the first intersection between the ray &amp; a surface that occurs at a $t$ in the interval $[t_0, t_1]$          sphere과 traingles 물체(surface)를 먼저 다루고, 다른 다면체들은 다음 섹션에 다루겠음      4.1. Ray-Sphere Intersection다음의 ray와 surface가 만나는 교차점을 구한다고 생각해봅시다,  ray $p(t) = \\mathrm{e} + t\\mathrm{d}$  implicit surface $f(p) = 0$\\[\\begin{align} f(p(t)) = 0 \\\\  f(\\mathrm{e} + t\\mathrm{d}) = 0 \\end{align}\\]      그리고 sphere는 중심점 $c = (x_c, y_c, z_c)$와 반지름 $R$을 이용해서 아래처럼 implicit equation을 나타낼 수 있음\\(\\begin{align} (x - x_c)^2 + (y - y_c)^2 + (z - z_c)^2 - R^2 = 0 \\\\  (p-c) \\cdot (p-c) - R^2 = 0    \\end{align}\\)              이 vector form 구의 방정식에서 p에다가 ray $p(t)$를 집어넣어도 식이 성립해야 함, intersection point이기 때문      $(\\mathrm{e} + t\\mathrm{d} - c) \\cdot (\\mathrm{e} + t\\mathrm{d} - c) - R^2 = 0$                  식을 풀면 아래와 같이 $t$에 대한 2차방정식 term으로 풀어짐      $(d \\cdot d)t^2 + 2d \\cdot (e-c)t + (e-c) \\cdot (e-c) - R^2 = 0$            근의 공식을 이용하여 $t$를 구한다\\(t = \\frac{-d \\cdot (e-c) \\pm \\sqrt{(d \\cdot (e-c))^2 - (d \\cdot d)( (e-c) \\cdot (e-c) - R^2)}}{(d \\cdot d)}\\)        근의 공식 풀기 위해서는  판별식 ($B^2 - 4AC$) 이거를 통해 근이 몇 개 나오는지 먼저 판단한다, 이 값이 음수면 허수라서 교차점 없다고 판단하면 됨              section 2.5.4에서 다루었던 것처럼, point $p$에 있는 normal vector는 gradient를 통해 아래처럼 계산할 수 있음      \\mathrm{n} = 2(p-c)        unit normal = (p-c) / R4.2. Ray-Traingle Intersection  barycentric 좌표계 사용함 =&gt; 삼각형을 포함하는 parametric plane을 표현할 때 삼각형의 꼭짓점만 이용하면 다른 storage가 필요없는 효율적인 좌표계표현이기 때문  parameteric surface와 ray간의 intersection point구하는 방법 :          Cartesian coordinate (데카르트 좌표계)를 이용한 다음 연립방정식으로 푼다=&gt; 식이 세 개이고, 구해야 되는 미지수도 $(t, u, v)$로 세 개이기 때문에 계산 가능      왼쪽 그림과 같이, $a,b,c$ 의 vertices로 이루어진 삼각형이 존재하고 ray $p(t) = \\mathrm{e} + t\\mathrm{d}$가 존재하는 상황을 가정할 때, intersection point인 $p$는 그림과 같이 ray 연장선 위에 존재한다. $$ \\mathrm{e} + t\\mathrm{d} = \\mathrm{a} + \\beta(\\mathrm{b} - \\mathrm{a}) + \\gamma(\\mathrm{c}-\\mathrm{a}) $$ 이 식에서 $t, \\beta, \\gamma$를 구해야 함위의 식을 vector form으로 아래처럼 식을 확장할 수 있음 :\\[\\begin{align}x_e  + tx_d = x_a + \\beta(x_b - x_a) + \\gamma(x_c - x_a), \\\\y_e  + ty_d = y_a + \\beta(y_b - y_a) + \\gamma(y_c - y_a), \\\\z_e  + tz_d = z_a + \\beta(z_b - z_a) + \\gamma(z_c - z_a).\\\\\\end{align}\\]      vector form을 행렬로 바궈서 standard linear system(선형 결합 형태)로 아래처럼 바꿔 표현 가능:        그 후, 크래머 규칙(Cramer’s rule)을 이용해서 $t, \\beta, \\gamma$ 값을 도출할 수 있다 (자세한 풀이 생략 책 참고)  5. Shading  픽셀에 대한 intersection point, 즉 visible surface가 구해졌다면, 광원을 고려해서 pixel color(intensity) 결정하는 단계  Light Reflection(반사) 관련 모델링을 사용한다          중요한 변수들                  light direction $\\mathrm{l}$          view direction $\\mathrm{v}$          surface normal $\\mathrm{n}$                    5.1. Lambertian Shading  가장 간단한 shading modeling 을 위한 가정**Lambertian Shading**  : 왼쪽 그림에서 표면에 떨어지는 광원으로부터 나오는 빛의 양은  빛에 대한 입사각 $\\theta$에 의해서만 결정된다. (View-Independent) 이를 이용해서 lambertian Shading model식을 아래처럼 설계  $$ L = k_d I max(0, \\mathrm{n} \\cdot \\mathrm{l}) $$  $k_d$ : diffuse coefficient (난반사 계수) 또는 surface color = 표면이 입사된 빛을 얼마나 균일하게(난반사로) 반사하는지를 나타내는 계수  $I$ : intensity of the light source  여기서 n과 l은 크기가 1인 단위벡터이기 때문에, $\\mathrm{n} \\cdot \\mathrm{l}$을 $cos\\theta$ 로 계산 가능하다.          즉, 내적은 cosine 유사도를 의미하므로 빛이 들어오는 방향에 대해서 얼마나 반사되는지 그 강도(intensity)가 결정된다는 의미      =&gt; 위의 모델링 수식은 RGB 채널 3개에 대해서 각각 적용되어 pixel value가 구해진다  $\\mathrm{v}, \\mathrm{l}, \\mathrm{n}$이 모두 단위벡터(크기가 1)인 것을 잊지 말기!5.2. Blinn-Phong Shading  모든 광원이 난반사(diffuse)로만 구성되지는 않는다,  specular component (정반사되는 성질) 빛을 모델링하기 위한 모델  Idea : $\\mathrm{v}$랑 $\\mathrm{l}$ 이 surface normal $\\mathrm{n}$을 가로지르는 상황에서 반사가 가장 잘된다는 것          mirror reflection이 발생할때 반사율이 가장 크다.      오른쪽 그림처럼, $\\mathrm{v}$와 $\\mathrm{l}$ 각도 중간에 위치한 half vector $\\mathrm{h}$가 있다고 가정해보자, 이 h가 surface noraml n과 가까울수록, specular component가 증가한다. (밝기가 증가) $\\mathrm{h}$와 $\\mathrm{n}$사이의 유사도는 **dot product(내적)**으로 생각할 수 있다. *Phong exponent*라고 불리는 power $p$가 surface에 대한 광택의 정도를 조절하는 역할임\\[\\begin{align}h = \\frac{\\mathrm{v} + \\mathrm{l}}{||\\mathrm{v} + \\mathrm{l}||}, \\\\L = k_d  I  max(0, \\mathrm{n} \\cdot \\mathrm{l}) + k_s I max(0, \\mathrm{n} \\cdot \\mathrm{h})^p \\\\\\end{align}\\]where, $k_s$ is the specular coefficient, or the specular color of the surface5.3. Ambient Shading      조명이 도달하지 않는 표면에서는 완전하게 black으로 보이기 때문에, 이런 현상을 줄이기 위해서 constant component를 shading model에 추가시킴        Full version of a simple and useful shading model(Ambident shading *components &amp; *Blinn-Phong model) :\\[\\begin{align}l = k_aI_a + k_dImax(0, \\mathrm{n} \\cdot \\mathrm{l}) + k_s I Max(0, \\mathrm{n} \\cdot \\mathrm{h})^n\\\\  \\end{align}\\]  "
  },
  
  {
    "title": "2D Gaussian Splatting, SIGGRAPH 2024",
    "url": "/posts/2DGS/",
    "categories": "Paper Review",
    "tags": "Geometry Reconstruction, Mesh extraction, Novel View Synthesis, Graphics",
    "date": "2025-02-03 16:34:00 +0900",
    





    
    "snippet": "Intro  3DGS : Redering결과 Inconsistency한 depth발생, 왜냐하면 pixel ray사이의 intersection을 통해 gaussian value를 구하기 때문  2DGS : explicit ray-splat intersection 이용  surface normal 정확하게 추출하면 mesh도 잘 구해짐          ...",
    "content": "Intro  3DGS : Redering결과 Inconsistency한 depth발생, 왜냐하면 pixel ray사이의 intersection을 통해 gaussian value를 구하기 때문  2DGS : explicit ray-splat intersection 이용  surface normal 정확하게 추출하면 mesh도 잘 구해짐                  depth distortion        : 2D primitives에 집중해서 더 좁은 범위의 ray에 가우시안들이 분포하게 해줌                    normal consistency        : rendered normal map과 rendered depth의 gradient 사이의 discrepancies 를 최소화함              두 개 regularaization term을 이용해서 smoother surface 구함  Contributions  efficient differentiable 2D Gaussian Renderer  surface recon을 위한 2가지 정규화 텀 소개 (depth distortion, normal consistency)  sota Reconstruction &amp; NVS result1. Modeling  normal = the steepest change of density          better alignment with thin surfaces        input = only a sparse calibration point cloud &amp; photometric supervision2D Gaussian Explicit Properties  central point $p_k$  two principal tangential vector $t_u, t_v$      scaling vector $S = (s_u, s_v)$    primitive normal : $t_w = t_u \\times t_v$      3x3 Rotation matrix $R = [t_u, t_v, t_w]$    2D Gaussian들은 local tangent plane에 정의됨      $P(u,v) = p_k + s_ut_uu + s_vt_vv =H(u,v,1,1)^T$ ⇒  world space에서 정의된 local tangent plane에서의 2D Gaussian  point    where $H$   is the homogeneous transformation matrix, representing the geometry of the 2D Gaussian        $G($u$) = exp(-\\frac{u^2+v^2}{2})$    $point - \\mathrm{u} = (u, v)$  2. Splatting  image space로 2D Gaussian들을 project하는 과정  affine approximation of Perspective Transformation 이용          Perspective transformation은 평행성이 지켜지지 않음, parallel한 line들이 소실점(vanishing point)에서 만남              한계점(선행 조건필요) :                              center of the Gaussian이 정확해야하고,                                center로부터의 거리가 멀어질수록 approximation error가 증가해야함                                homogeneous coordinate을 이용해서 완화된 해결방법 제안됨      2D Splat을 2D image plane에 projection한느 건 일반적인 2D-to-2D mapping in homogeneous coordinate이라고 할 수 있다    $W$ :  transformation matrix from World space to Screen space      Screen space에서의 projected points $\\mathrm{x}$    $\\mathrm{x} = (xz, yz, z, 1)^T = WP(u,v) = WH(u,v,1,1)^T$ 로 구해짐    2D Gaussian들을 rasterize하려면, inverse transfomation 인 $M= (WH)^{-1}$을 이용하는 방법이 있으나, 이 방법은  numerical instability함 특히 splat이 line segment(view가 옆면에서 보이는 상황인 경우, )에서 특히 결함이 많음      이 문제 해결위해 previous surface splatting rendering method들은 미리 정의된 predefined threshold을 이용해서  ill-conditioned transformation을 이용해왔음 (related work)      ⇒ 모두 unstable함, 따라서 본 논문에서는 “Ray-Splat Intersection” 기반 방법을 제안2-1. Ray-Splat Intersection      평행하지 않은 3가지 평면 (three non-parallel planes) 에서의 교점을 찾음으로써 ray-splat intersection을 위치시킴  (??)        image space coordinate $\\mathrm{x} = (x, y)$가 주어졌을 때,    두개의 수직 plane (x-plane과 y-plane)의 교차 ray를 파라미터화한다            x - plane : normal vector인 (-1, 0, 0)과 offset x로 정의됨    ⇒ 4D homogeneous Plane (homogeneous coordinate을 이용한 4D 좌표계 표현한 개념)    $h_x = (-1, 0, 0, x)^T$        y - plane : normal vector인 (0, -1, 0)과 offset y로 정의됨    ⇒ $h_y = (0, -1,  0, y)^T$        ray $\\mathrm{x} = (x,y),$  ( Image Coordinate ) 는 x-plane과 y-plane사이의 교차되는 line에 대한 좌표로 결정    각각의 plane을 2D Gaussian의 local coordinate( $uv-$coordinate system)으로 변환해야 함  point로부터 plane으로의 transformation matrix $M$ 의 역행렬을 곱해주는 Inverse Transpose $M^{-1}$을 수행해야 intersection of the x&amp;y-plane에 대한 좌표를 아래처럼 구할 수 있음,                  이 때 $M = (WH)^{-1}$, $M^{-T} =  (WH)^T$\\[\\begin{align} h_u = (WH)^Th_x   \\\\ h_v = (WH)^Th_y \\end{align}\\]        where, W : world space에서 screen space로 가는 변환 행렬, H : homogeneous 좌표계로 변환하는 행렬              두 평면이 만나는 intersection point $\\mathrm{u}(\\mathrm{x})$  구하기                  2D Gaussian의 point  : $(u, v, 1, 1)$        $h_u^i , h_v^i$ 는 4D homogeneous plane 파라미터들 중에서 i번쨰 파라미터를 의미함          \\[\\begin{align}h_u \\cdot (u,v,1,1)^T = h_v \\cdot (u,v,1,1)^T = 0  \\\\ u(\\mathrm{x}) = \\frac{h_u^2h_v^4-h_u^4h_v^2}{h_u^1h_v^2-h_u^2h_v^1} \\\\ v(\\mathrm{x}) = \\frac{h_u^4h_v^1-h_u^1h_v^4}{h_u^1h_v^2-h_u^2h_v^1} \\end{align}\\]        Screen space에서의 projected points    $\\mathrm{x} = (xz, yz, z, 1)^T = WP(u,v) = WH(u,v,1,1)^T$    이거 식을 Recall해서 $x, y, u, v$ 다 아니까 depth $z$를 구할 수 있다.  3. Improvements3-1. Degenerate Case  object-space low-pass filter 제안\\[\\begin{align}    \\widehat{G}(\\mathrm{x}) = \\max \\left\\{ G(\\mathrm{u}(\\mathrm{x})), G\\left(\\frac{\\mathrm{x}-c}{\\sigma}\\right) \\right\\}\\end{align}\\]⇒ 직관적인 수식 의미 = fixed screen-space(오른쪽 항)에서의 Gaussian이랑 위에서 구한 intersection point u(x)에서의 가우시안중에 더 큰 값을 선택하여 degenerate하는 가우시안들 없앰where $c$  : projection of center $p_k$본 실험에서는 $\\sigma = \\sqrt{2}/2$로 설정함      Volumetric alpha blending (Rasterization)\\[c(\\mathrm{x}) = \\sum_{i = 1}c_i\\alpha_i\\widehat{G}_i(\\mathrm{u}(\\mathrm{x}))\\prod_{j =1}^{i-1}(1-\\alpha_i \\widehat{G}_i(\\mathrm{u}(\\mathrm{x})))\\]  4. Training  3DGS의 photometric loss에다가 2가지 정규화 텀을 추가시켜서 더 smooth한 surface align을 촉진하여 geometry reconstruction의 성능 향상4.1. Depth Distortion      Depth distortion loss : ray-splat intersection결과상 구한 depth 간의 거리 차를 최소화함으로써 ray에 존재하는 weight을 더 concentrate해줌,  Mip-Nerf360에서 영감 받음\\[\\begin{align} L_d = \\sum_{i,j}w_iw_j|z_i-z_j| \\end{align}\\]    where, $w_i :$i번째 intersection하는 지점에서의 blending weight    $z_i :$ i번째 교차점에서의 depth  4.2. Normal Consistency  교차하는 median point(중간 지점) $p_s$에서 actual surface 고려      축적된 opacity가 0.5가 되면, splat된 가우시안들의 normal과 depth map에서의 gradient를 고려해서 align해주는 과정\\[\\begin{align}L_n = \\sum_iw_i(1-n_i^T\\mathrm{N}) \\end{align}\\]    where $i :$ ray상에 존재하는 intersected 된 splat들    $w :$ blending weight of the intersection point    $n_i :$ normal of the splat that is oriented towards the camera    $\\mathrm{N} :$ normal estimate by the gradient of the depth map  Final Loss: $L = L_c + \\alpha L_d + \\beta L_n$$L_c :$ RGB reconstruction loss combining L1 with the D-SSIM term𝛼 = 1000 for bounded scenes, 𝛼 = 100 for unbounded scenes, and 𝛽 = 0.05 for all scenes.5. Experiments  single RTX 3090  Datasets          DTU                  15개 씬으로 구성됨 (49장 또는 69장 이미지, resolution = 1600 x 1200)          Colmap으로 sparse point cloud얻어진 상태, 해상도를 800 x 600 으로 downsample한 이후에 사용함 효율성을 위해서                    TnT      MipNerf360        Evaluation Metrics          PSNR      SSIM      LIPPS      Limitations  assume surfaces with full opacity and extract meshes from multi-view depth maps          semi-transparent한 표면에서는 잘 복원이 안됨, 유리같은 부분        fine geometric structure에 대해서는 덜 정확함  regularization term사용할 때 이미지 퀄리티와 geometry간의 trade-off가 발생함 ⇒ 특정 영역에서 over-smooth결과 초래할 수 있음"
  },
  
  {
    "title": "Graphics-Ch3. Raster Images",
    "url": "/posts/ch3-Raster/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-01-29 18:00:00 +0900",
    





    
    "snippet": "Chapter 3. Raster Images서론  Raster image : 2D 행렬 형태로 픽셀에 대한 RGB color값을 표현한 이미지          RGB 색상은 3차원 벡터 형태        디지털 카메라는 image sensor를 포함하고 있는데, 이것은 빛이 들어오는 정도(intensity)와 color를 구성하고 있음  하지만 우리가...",
    "content": "Chapter 3. Raster Images서론  Raster image : 2D 행렬 형태로 픽셀에 대한 RGB color값을 표현한 이미지          RGB 색상은 3차원 벡터 형태        디지털 카메라는 image sensor를 포함하고 있는데, 이것은 빛이 들어오는 정도(intensity)와 color를 구성하고 있음  하지만 우리가 이러한 2D array자체로 이미지를 보지는 않음,          이미지 픽셀과 디스플레이 픽셀간의 매칭(direct link)이 필요하고 이를 Rasterizer가 처리함        Vector Image : 픽셀에 대한 행렬형태로 나타내는 것이 아니라, shape이랑 line, curves로 경계지어진 color 영역에 대한 이미지 표현          resolution independent하다는 점과, 고해상도로 displayed된다는 점이 장점이지만 display되기 이전에 rasterized되어야 한다는 점이 단점이다.        이번 챕터에서는 Raster Image를 다루며, 빛의 세기(light iintensity)와 연관된 pixel value를 어떻게 결정하는지에 대한 자세한 내용은 나중 챕터에서 다룰 예정이니 일단 기억해두기.1. Raster Devices  장치를 Raster한다는 것은 무슨 의미일까  input output을 간단한 조직도로 나타낸 사진은 아래와 같다.1.1. Displays  pixel value에 대한 고정된 2D array 기반으로 디스플레이되어지는데, 방식이 두 가지로 분류 가능함 1) Emissive(방출하는) Display : 픽셀들이 direct하게 조절가능한 light를 방출하는 방식   픽셀 자체가 광원이 됨  LED(Light-Emitting Diode)가 대표적인 예시2) Transmissive(투과하는) Display : 빛을 방출하진 않고, 대신에 투과할 수 있는 만큼의 빛을 픽셀이 갖고 있는 방식  transmissive display는 픽셀 행렬 뒤에 backlight가 필요함, 어느정도 illuminate(밝게 하다)될 만큼의 광원이 필요하기 때문  LCD(Light-Crystal Display)가 대표적인 예시    on-state(위, 전압 들어온 상태)가 되면 liquid crystal cell이 회전하면서 polarized light이 front plarizer를 투과할 수 있게 해주는 원리2. Images, Pixels, and Geometry  Raster Image는 이미지 픽셀별로 RGB 색상에 대한 값을 2차원 행렬 형태로 나타내는 것을 알고 있다.  이미지를 측정하거나 reproduce할 때, light energy에 대해 다음의 2차원 분포를 알아야 한다 ;                  light emitted from the monitor as a function of a position on the face of the display  = 디스플레이 표면에서 방출되는 빛                    light falling on a camera’s image sensor as a function of a position across the sensor’s plane  = 카메라의 이미지 센서로 들어가는 빛                    Reflectance(반사율) 또는 흡수되는 빛 대비 반사되는 빛의 비율(fraction)  = function of position on a piece of paper            We can abstract an “image” as a function $I(x,y)$: \\(I(x,y) : R -&gt; V\\)          grayscale 이미지인 경우에는 $V$가 양수구역이고, ideal한 color image인 경우에 $V$는 3차원 실수 공간영역이다.        continuous(연속적인)값으로 어떻게 Raster image가 표현되는 것인가.          Point sampling과 관련있다, 자세한 것은 chapter 9의 신호처리단원에서 다룬다고 한다      이미지의 색상에 대한 Local average를 pixel value정함      pixel value인 x를 구한다는 말은 즉, “the value of the image in the vincinity(주변) of the grid point is x”를 구한다는 것과 같은 말이다. (당연한거아님?)        이미지 width가 $n_x$, 높이가 $n_y$일 떄, image rectangular domain은 다음과 같이 평행이동을 0.5씩 했다면 top-right pixel도 똑같이 평행이동시킨 만큼으로 설정할 수 있다;\\(R = [-0.5, n_x - 0.5] \\times [-0.5, n_y - 0.5]\\)2.1. Pixel Values  Pixel formats :          비트 수가 줄어들면, artifact나 flaws(결함)부분이 이미지상에 생길 수도 있다      2.2. Monitor Intensities and Gamma  모니터는 pixel “value”를 digital input으로 받아서, “intensity” level(빛의 강도)로 변환함  인간의 강도에 대한 인지는 비선형적이라서 이 단원에서 논할 부분은 아님(chapter 20에 나옴)  모니터도 input에 대해서 non-linear하게 처리하는데, 예를 들어 모니터에게 0, 0.5, 1.0의 세 픽셀 value를 주었을 때, 디스플레이되는 intensity는 0, 0.025, 1.0으로 처리됨      이러한 근사 비선형성은 $ \\gamma $에 의해 결정됨\\(displayed-intensity = (maximum-intensity) a^\\gamma\\)    이 때, a는 아래 그림의 체커보드 이미지를 통한 standard technique을 통해 찾을 수 있는 값임 (calibration과 연관된 건가?)  “Gamma Correct”          gamma값을 알고 있으면, input을 감마 변형해서 $a = 0.5$ 즉, 흑백의 중간 강도에 대해 디스플레이되게 할 수 있음, 아래의 변형과정을 거친다      $a’ = a^{\\frac{1}{\\gamma}} $      \\[: displayed-intensity = (a')^{\\gamma} = (a^{\\frac{1}{\\gamma}})^{\\gamma}(maximum-intensity) = a(maximum-intensity)\\]            느낀점  pixel 정의나 이미지에 대한 matching function같은 기초적인 개념을 훑고 가는 시작 단원이다,, 별 내용이 없음  설날인데 여유롭게 카공도 하고 기분이 좋다.ㅋㅋ"
  },
  
  {
    "title": "Graphics STUDY",
    "url": "/posts/Graphics_study/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-01-25 11:00:00 +0900",
    





    
    "snippet": "Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley  교수님 추천 책, (“그래픽스에서 가장 유명한, 정석 같은 책이에요. 그래픽스의 거의 모든 분야 다룸”)  아직 차례만 보긴 했는데 깊게 알고싶었던 내용 총집합인 것 같아서 아주 마음에 든다Contents1. Introduc...",
    "content": "Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley  교수님 추천 책, (“그래픽스에서 가장 유명한, 정석 같은 책이에요. 그래픽스의 거의 모든 분야 다룸”)  아직 차례만 보긴 했는데 깊게 알고싶었던 내용 총집합인 것 같아서 아주 마음에 든다Contents1. Introduction  1.1 Graphics Areas  1.2 Major Applications  1.3 Graphics APIs  1.5 Numerical Issues  1.6 Efficiency  1.7 Designing and Coding Graphics Programs 2. Miscellaneous Math  2.1 Sets and Mappings  2.2 Solving Quadratic Equations  2.3 Trigonometry  2.4 Vectors  2.5 Curves and Surfaces  2.6 Linear Interpolation  2.7 Triangles 3. Raster Images  3.1 Raster Devices  3.2 Images, Pixels, and Geometry  3.3 RGB Color  3.4 Alpha Compositing 4. Ray Tracing  4.1 The Basic Ray-Tracing Algorithm  4.2 Perspective  4.3 Computing Viewing Rays  4.4 Ray-Object Intersection  4.5 Shading  4.6 A Ray-Tracing Program  4.7 Shadows  4.8 Ideal Specular Reflection  4.9 Historical Notes5. Linear Algebra  5.1 Determinants  5.2 Matrices  5.3 Computing with Matrices and Determinants  5.4 Eigenvalues and Matrix Diagonalization 6 Transformation Matrices  6.1 2D Linear Transformations  6.2 3D Linear Transformations  6.3 Translation and Affine Transformations  6.4 Inverses of Transformation Matrices  6.5 Coordinate Transformations7 Viewing  7.1 Viewing Transformations  7.2 Projective Transformations  7.3 Perspective Projection  7.4 Some Properties of the Perspective Transform  7.5 Field-of-View8. The Graphics Pipeline  8.1 Rasterization  8.2 Operations Before and After Rasterization  8.3 Simple Antialiasing  8.4 Culling Primitives for Efficiency9. Signal Processing  9.1 Digital Audio: Sampling in 1D  9.2 Convolution  9.3 Convolution Filters  9.4 Signal Processing for Images  9.5 Sampling Theory10. Surface Shading  10.1 Diffuse Shading  10.2 Phong Shading  10.3 Artistic Shading11. Texture Mapping  11.1 Looking Up Texture Values  11.2 Texture Coordinate Functions  11.3 Antialiasing Texture Lookups  11.4 Applications of Texture Mapping  11.5 Procedural 3D Textures12 Data Structures for Graphics  12.1 Triangle Meshes  12.2 Scene Graphs  12.3 Spatial Data Structures  12.4 BSP Trees for Visibility  12.5 Tiling Multidimensional Arrays20단원까지 있는데 일단 리버털 끝나고나서 2월 말까지 여기까지 1회독 목표 학부 때 배운 패턴인식이랑 컴비때 배운 내용이 포함된게 많아서 복습할겸 모르는 내용 위주 공부해야겠다."
  },
  
  {
    "title": "MarchingCubes, SIGGRAPH 1987",
    "url": "/posts/MarchingCubes/",
    "categories": "Paper Review",
    "tags": "Mesh extraction, Graphics",
    "date": "2025-01-21 11:00:00 +0900",
    





    
    "snippet": "Abstract  output = 연속적인 density값을 갖는 surface의 triangle model  알고리즘 큰 흐름 : 3D medical data를 scan-line order에 처리한 후, 선형 보간(linear interpolation)을 이용해서 삼각형의 vertices(꼭짓점)을 구하는 것Introduction  mesh extr...",
    "content": "Abstract  output = 연속적인 density값을 갖는 surface의 triangle model  알고리즘 큰 흐름 : 3D medical data를 scan-line order에 처리한 후, 선형 보간(linear interpolation)을 이용해서 삼각형의 vertices(꼭짓점)을 구하는 것Introduction  mesh extraction하는 것은 medical image에서 굉장히 유용하게 많이 쓰임  새로운 3D surface construction 알고리즘인 “Marching Cube”는 연속적인 density surface를 가지는 물체에 대한 vertices를 추출함으로써 고해상도의 메쉬 만듦  3D Medical 알고리즘 흐름 :                            Data Acquistion          MR, CT, SPECT같은 기기로 환자 데이터 얻음                                      Image Processing          3D data의 전체적인 구조를 파악할 수 있는 기법 사용                                      Surface Construction          본 논문의 주제, 적절한 알고리즘 사용                    Display      Related Work      기존의 연구 (1) Connected Contour 알고리즘 : surface의 윤곽(contour)에서 시작하여 그것들이 서로 연속적인 삼각형이 되게 연결하는 접근법   -&gt; slice에 하나 이상의 contour가 존재해야하는데 이때 ambiguity(모호성)이 발생하여 정확도 떨어짐   -&gt; 원데이터의 inter-slice의 연결성을 무시함    (2) Cuberille 접근법 : cuberilles(작은 큐브인 voxel형태로 표현하는 구조)로부터 surface 구축하는 접근법   -&gt; 이 구조에서 gradient를 계산했을 때 그 값이 shading(그림자)영역의 지점을 찾는데에 이용되는데, 이게 정확하기가 쉽지 않음   -&gt; thresholding해서 3D space를 블록 단위 voxel처럼 표현하고 surface을 표현    (3) Ray casting   -&gt; 3차원 sensation을 생산하기 위해 motion에 의존함(?)   Marching Cubbe Algorithm (Method)  크게 2가지의 주요한 단계로 구성됨  divide-and-conquer 접근법  3D space상의 큐브 하나에서 다음 큐브로 넘어갈 때, surface가 어떻게 교차하는지를 찾는 것이 목표      이웃한 8개 픽셀(두 Slice 면의 4개 꼭짓점)로부터 logical한 cube가 Figure-2처럼 위치하게 세팅,        큐브의 꼭짓점(vertex) 데이터 값이 우리가 구성하는 surface의 vertex value값을 초과하면 1을 부여  마찬가지로 큐브 vertex value가 surface vertex value값보다 아래이면 0을 부여=&gt; surface의 외부에 존재하는 점 대략 이러한 과정으로 교차점들을 통해 surface를 복원하게 된다. 큐브마다 8개의 vertices &amp; 2개의 state(inside, outside)가 존재하기 때문에, 표면이 꼭짓점 1개 당 $2^8$가지 경우의 수로 교차될 수 있다.  해당 256가지 경우의 수에 대한 table을 만들 수 있으나 매우 따분하고 에러가 발생하기 쉽기 때문에 , 256가지 경우의 수를 14가지 패턴으로 줄일 수 있는 다음의 두 가지 대칭 속성을 이용함          큐브가 reverse로 뒤집혀도, surface value들은 그대로 동일하게 바뀌지 않음              Rotational 대칭                        ‘’‘(ex) 0번 패턴 : 모든 vertices가 0으로 선택된 케이스(혹은 모두 1) =&gt; 삼각형을 생산하지 않게 됨          1번 패턴 : surface가 1개의 vertex를 나머지 7개의 vertices에 대해 분리시킨 상태 =&gt; 작은 삼각형 1개           ….  이 때, 윗 그림처럼 8개 vertices(v1~v8)와 각 vertex 사이의 bit index 12개(e1~e12)를 numbering하여 edge intersection을 고려한다.  그 후 surface와 맞닿는 edge가 어떤 것인지 알았으면, 이 edge들 사이에서 linear interpolation(선형 보간)을 수행한다.  마지막 단계로 각 triangle vertex에 대한 unit normal(단위 법선벡터)를 계산한다.          이렇게 구한 normal로 Gouraud-shaded image를 렌더링할 때 사용됨, 즉 명암 넣기 단계임                  고러드 쉐이딩(Gouraud Shading)                    Details :                  surface 의 normal은, surface의 접선 방향에 대한 gradient vector이다.                      direction of gradient vector를 $\\vec{g}$로 표기\\[\\vec{g}(x,y,z) = \\Delta{\\vec{f}(x, y,z)}\\]                                    $\\Delta{\\vec{f}(x,y,z)}$ 를 구하기 위해 3방향에서의 gradient vector를 아래처럼 계산한 후에 선형보간을 하여 surface 복원    \\(G_x(i,j,k) = \\frac{D(i+1, j, k) - D(i-1, j, k)}{\\Delta{x}}\\)     \\(G_x(i,j,k) = \\frac{D(i, j+1, k) - D(i, j-1, k)}{\\Delta{x}}\\)    \\(G_x(i,j,k) = \\frac{D(i, j, k+1) - D(i, j, k-1)}{\\Delta{x}}\\)        In summary, marching cubes creates a surface from a three-dimensional set of data as follows: (논문 표현)          Read four slices into memory.      Scan two slices and create a cube from four neighbors on one slice and four neighbors on the next slice.      Calculate an index for the cube by comparing the eight density values at the cube vertices with the surface con- stant.      Using the index, look up the list of edges from a precal- culated table.      Using the densities at each edge vertex, find the surface- edge intersection via linear interpolation.      Calculate a unit normal at each cube vertex using central differences. Interpolate the normal to each triangle ver- tex.      Output the triangle vertices and vertex normals      Enhancements to the Basic Algorithm  "
  },
  
  {
    "title": "SuGaR, CVPR 2024",
    "url": "/posts/SuGaR/",
    "categories": "Paper Review",
    "tags": "Mesh reconstruction, 3D Gaussian Splatting",
    "date": "2025-01-20 11:00:00 +0900",
    





    
    "snippet": "Abstract  precise and extremely fast mesh extraction from 3DGS representation  state-of-the-art method on SDFs, while providing a better rendering qualityContributions  Regularization term         ...",
    "content": "Abstract  precise and extremely fast mesh extraction from 3DGS representation  state-of-the-art method on SDFs, while providing a better rendering qualityContributions  Regularization term          Gaussian Splatting 최적화 중의 loss term에 추가시켜서 3D Gaussian들이 표면에 잘 정렬되게 하는 역할      3D Gaussian의 surface geometry(density &amp; SDF)를 이용함        Refinement strategy          mesh에 있는 gaussian들 삼각형으로 묶어주면서 refinement함      Introduction  mesh extraction task는 3DGS representation의 explicit한 특성 때문에 더 어려움  3DGS가 잘 최적화되었으면, 가우시안들이 평평하고 표면에 잘 분포되어있다는 가정을 얻을 수 있음          이때 gaussian density와 관련된 geometry를 이용해서 Loss term에 추가하면서 gaussian으로부터 mesh추출이 더 쉬워지게 만듦      volumne density 사용?        Marching Cube 알고리즘 대신 Poisson Reconstruction 알고리즘 사용해서 point cloud로부터 mesh extraction을 했다Method1. Aligning the Gaussians with the Surface (=Regularization)  목표 : 3DGS가 잘 최적화되어있다는 가정 하에, Gaussian의 SDF(Signed Distance Function)를 이끌어내는 것      optimized된 가우시안으로부터 예측된 SDF와 실제의 SDF간의 차이를 최소화함으로써 가우시안들이 평평하게 surface align된 특성을 갖을 수 있도록 encourage        최적화된 Gaussian Splatting Scene이 주어져있는 상황에서 시작    Gaussian의 density function $d(p)$ :\\[d(p) = \\sum_g \\alpha_g exp(-\\frac{1}{2}(p-\\mu_g)^T\\sigma_g^{-1}(p-\\mu_g))\\]  &lt; Property 1 &gt;1️⃣surface에 가까운 point $p$에 가까이 위치한 Gaussian $g^*$이 density function $d(p)$에 기여하는 정도가 크다\\[g^* = \\arg\\min_g(p-\\mu_g)^T\\sigma_g^{-1}(p-\\mu_g)\\]해석 = g*말고 나머지 가우시안들, g들에 대한 밀도가 최소가 되게 하면 된다.⇒ 이 성질을 만족하면, 가우시안들이 scene에서 잘 spread되어있다, 즉 scene에 가우시안들이 잘 퍼져있다는 가정을 만족&lt; Property 2 &gt;2️⃣잘 최적화된 3DGS scene에서는 Gaussian들이 평평하다 = scaling factor 3방향 벡터 중에서 하나는 0에 가까워야 함, (길이 짧아야 함)\\[(p-\\mu_g)^T\\Sigma_g^{-1}(p-\\mu_g) \\approx \\frac{1}{s_g^2}&lt;p-\\mu_g, n_g&gt;\\][notation]  $s_g$: 가장 짧은 scaling factor  $n_g :$ scaling factor에 대응되는 축에 대한 방향, normal(법선 벡터)처럼 생각해도 됨⇒ 결과적으로 surface-align한 density function $\\overline{d}(p)$\\[\\overline{d}(p) = exp(-\\frac{1}{2s_{g^*}^2} &lt;p-\\mu_{g^*}, n_{g^*}&gt;^2)\\]  A : d(p) 밀도함수를 따르는 가우시안,  B : $\\overline{d}(p)$ 밀도함수를 따르는 가우시안&lt; Optimize Term &gt;  $|d(p) - \\overline{d}(p)|$: 위의 density volume을 이용한 optimization term 을 3DGS loss에 추가          밀도를 이용하는 지금 optimize term 일단 좋긴 좋은데,,,density말고 SDF(Signed Distance Function) 활용하는 것도 추가시키면 surface-align Gaussian을 얻는 것에 더 좋다고 함    - 평평한 가우시안이 주어졌을 때, 즉 Gaussian $g$의 scaling factor들이 $s_g$ = 0 인 상황에서, point $p$와 true surface와의 거리 : $|&lt;p-\\mu_{g’}, n_{g’}&gt;|$      \\[SDF : \\overline{f}(p) = \\pm s_{g^*}\\sqrt{-2log(\\overline{d}(p))}\\]\\[ideal-SDF : {f}(p) = \\pm s_{g^*}\\sqrt{-2log({d}(p))}\\]   $|\\overline{f}(p)-f(p)|$ : 위의 SDF를 이용한 optimization term을 통해 표면에 더 잘 정렬된 가우시안들을 얻을 수 있었다.   Regularization term $R$ :\\[R =  \\frac{1}{|P|} \\sum_{p \\in P} |\\overline{f}(p) - f(p)|\\]      SDF의 normal(법선 벡터)에 대한 regularization term도 있음\\[R_{Norm} = \\frac{1}{|P|}  \\sum_{p\\in P} || \\frac{\\nabla f(p)}{|| \\nabla f(p) ||^2} - n_{g^*} ||_2^2\\]  2. Efficient Mesh Extraction (=Poisson Reconstruction)  최적화된 3DGS scene에서 계산된 가우시안들의 density로부터 3D points를 일정 level set에 대하여 샘플링함 =&gt; point clouds 구함                  이 때, level set은 level parameter인 $\\lambda$에 의해 결정됨                          샘플링된 Points 기반으로 Poisson reconstruction 수행하여 mesh 추출    참고. Poisson Reconstruction 정리 글  3. Binding New Gaussians to the Mesh (=Refinement)  Barycentric 좌표계 :  삼각형 또는 다면체 내부의 점을 해당 도형의 꼭짓점에 대한 가중치로 표현하는 좌표계          (ex) 삼각형의 경우:                              삼각형의 꼭짓점을 A, B, C라 할 때, 내부의 임의의 점 P는 다음과 같이 표현됩니다:            $P = \\alpha A + \\beta B + \\gamma C$                                여기서 α,β,γ는 가중치로, 아래 조건을 만족합니다:            $\\alpha + \\beta + \\gamma = 1$                                    논문 표현 :    Also, the Gaussians have only 2 learnable scaling factors instead  of 3 and only 1 learnable 2D rotation encoded with a complex number rather than a quaternion, to keep the Gaussians flat and aligned with the mesh triangles.        quaternion이란: 3D 회전을 표현하기 위해 사용되는 수학적 구조로, 복소수의 확장된 형태    $q=w+xi+yj+zk$  Experiment  single GPU Nvidia Tesla V100 SXM2 32 Go느낀점  3DGS에서 거의 최초로 mesh reconstruction태스크를 수행해서 성능이 좋게 나왔다는 것이 의의  요즘 모델들의 거의 baseline 시초급(?)으로 봐도 무방하다  포아송 재건방법이 아직 뭔지 잘 모르겠다. marching cube공부할 때 같이 공부  어렵긴 한데 재밌다."
  },
  
  {
    "title": "GS2Mesh, ECCV 2024",
    "url": "/posts/GS2Mesh/",
    "categories": "Paper Review",
    "tags": "Mesh reconstruction, Surface reconstruction, 3D Gaussian Splatting",
    "date": "2025-01-19 15:00:00 +0900",
    





    
    "snippet": "GS2Mesh :Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&lt; 선정 이유 &gt;  SuGaR (CVPR 2024)논문 이후로 유의미하게 3dgs의 mesh recon 태스크에서 성능이 sota달성하였다는 점,  baseline에 SuGaR가 존재한다는 점Abstra...",
    "content": "GS2Mesh :Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&lt; 선정 이유 &gt;  SuGaR (CVPR 2024)논문 이후로 유의미하게 3dgs의 mesh recon 태스크에서 성능이 sota달성하였다는 점,  baseline에 SuGaR가 존재한다는 점Abstract  noisy한 3DGS representation으로부터 smooth한 3D mesh representation을 얻는 것 어려움  pre-trained stereo-matching model사용해서 scene에 대한 geometry 활용함          stereo-aligned 되어있는 image pair를 얻고, 이를 앞선 stereo matching모델에 넣어서 depth추출하여 geometry 이용함        more smoother, more accurate mesh extraction 가능          Mesh Extraction 알고리즘 : TSDF(Truncated Signed Distance Function) 이용한 Marching cube      Contributions  pre-trained stereo-matching 모델을 통해 Image pair의 geometry 활용해서 mesh reconstruction 태스크의 sota성능을 달성하였다.          사용한 스테레오 매칭 모델 : DLNR                  stereo calibrated image pair를 모델 input으로 사용하여 이미지 쌍에 대한  correspondence 문제를 해결 + depth 추출                      TSDF(Truncated Signed Distance Function)와 depth 정보를 활용해서 mesh recon하는 알고리즘 (marching cube기반) 사용하였다. Introduction  Gaussian의 explicit한 요소만으로 geometrically consistent한 surface를 추출하는 것은 어려움          image plane(2D)에 back projected 되었을때 best matching되게끔 최적화된 가우시안이기 때문에, mesh reconstruction 에서는 오히려 가우시안 representation이 단점이 됨            stereo-aligned된 이미지 쌍에서 stereo-matching 모델(DLNR)을 사용해서 정확한 depth 측정한 후, TSDF 활용한 Depth-fusion기반 mesh extraction 알고리즘 (Marching cube) 사용하는 파이프라인    데이터셋 : TnT(Tanks and Temples) &amp; DTU(작은 object mesh 데이터셋임) 에서 sota성능을 보여주었다.MethodStep 1. Scene Capture &amp; Pose Estimation  COLMAP의 SFM(Structure From Motion)을 통해 카메라 파라미터들을 얻어내고, sparse한 3D point cloud를 재건한다.Step 2. Stereo-aligned Image Pairs 생성  3DGS에서 photometric loss를 통해 최적화되어서 image plane에 back-projected된 가우시안들 기반으로 stereo-aligned(같은 베이스라인b 선상에 존재하는, 카메라 포즈(rotation, translation)은 그대로에 baseline b길이만큼만 떨어진) 한 쌍으로 만드는 과정을 수행한다.          \\[R_R = R_L\\]\\[T_R = T_L + (R_L \\times [b, 0, 0])\\]   수식 Notation 설명   - $R_R$: 오른쪽 카메라의 rotation matrix    - $R_L$: 왼쪽 카메라의 rotation matrix    - $T_R$: 오른쪽 카메라의 translation matrix    - $T_L$: 왼쪽 카메라의 translation matrix  Step 3. Stereo Depth Estimation  input : a pair of stereo-calibrated cameras      DLNR(High Frequency Stereo matching Network) 모델 이용        DLNR pipeline          multiscale decouple LSTM 구조를 따름 + Disparity Normalization 수행하는 것이 핵심      논문까지 자세힌 아직 안읽어봄, 걍 stereo matching model 성능 중에 sota라고 함            이 모델 output에다가 아래의 mask 2개 정도 추가시켜 reconstruction성능 향상              occlusion mask                  left-to-right disparity랑 right-to-left disparity 차이 이 두개 사이에서 thresholding해서 구해짐                    depth-shading                  stereo-matching error $\\epsilon(Z)$        \\[\\epsilon(Z) = \\frac{\\epsilon(d)}{f_x \\cdot B} Z^2\\]                  $Z$ :  ground-truth depth          $d$ : disparity          $f_x$ : 수평축 카메라 focal length       - error가 baseline B 길이 값이 커질수록, 즉 stereo-paired image 사이의 간격이 클수록 에러가 작아지는 반면, occlusion이 심해질 수 있음          4B ≤ Z ≤ 20B 에 속하는 깊이만을 고려함                    Step 4. Depth Fusion into Triangulated Surface (mesh)  추출된 depth 정보들을 TSDF(Truncated Signed Distance Function) 기반 mesh reconstrucction 방법에 통합시킨다                  TSDF Cube Model이란?        : 깊이 영상 으로부터 3차원 공간 표면을 효과적으로 표현하기 위해,        전체 공간을 일정한 크기의 정육면체 복셀(voxel)들로 구성된 커다란 하나의 큐브(cube)로 표현하고, 각 복셀에는 물체 표면과의 거리를 나타내는 TSDF값과 그 값의 신뢰도를 나타 내는 가중치(weight)를 함께 저장하는 방식                    etc에 TSDF 개념 간단하게 추가              Marching cube : mesh 만들어주는 알고리즘          sdf나 tsdf같은 함수 활용되며, surface representation 방식 기반으로 mesh 뽑아냄      Experiment  ground truth point cloud랑 reconstructed point cloud간의 Chamfer Distance(CD)계산을 통해 evaluation  Baseline : SuGaR(CVPR 2024), BakedSDF, Neuralangelo, VolSDF, NeuS, MVSformer,  데이터별로 실험 진행 얘기  평가 지표 : Chamfer-Distance(CD): 두 point cloud 집합간의 거리 측정, F1, AccuracyLimitation  오른쪽 스테레오 매칭 모델은 투명한 표면에서 어려움을 겪는다.  (왼쪽) 원래 학습 이미지에서 충분히 다루어지지 않은 영역에서 floater를 생성한다.  TSDF 퓨전은 큰 장면(넓은 baseline B)에 맞게 확장되지 않는다.etc., (Preliminaries)기초개념 정리기초 개념      Mesh란?    : 3D 공간상에 존재하는 점들(Vertex/ Point) 과 그 점 3 개의 집합인 면(Polygon/face)들로 이루어진 3D 공간 표현방법        Voxel이란?    :이미지의 pixel 처럼 3D 공간을 표현하기 위해서 3D공간을 작은 단위 공간으로 쪼갠 것 –&gt; 3차원 공간을 grid로 쪼갰다고 보면 편함  TSDF (Truncated Signed Distance Function)  3D scene reconstruction의 목적은 Surface 를 찾아 recon 하는 것인데 이때 surface 를 표현하는 함수를 SDF(Signed Distance Function) 라고 함      Voxel 형태로 단위공간을 나누어 surface 라고 판단되는 곳은 0, Surface 안쪽은 음수 , Surface 바깥쪽은 양수로 표현하는 방식      Marching Cube (SIGGRAPH 1987)  과정 다 생략하고 한 줄 요약 : 3D point cloud로부터 Mesh 생성 알고리즘2차원 image plane에서 물체가 빨간색 선처럼 생겼다고 생각해보자, (3차원에서는 voxel임)이것을 2차원 도트로 표현하면 아래와 같음⇒ 원형의 물체랑 너무 달라짐, 해상도 차이 발생, 모양 이상해지는 결과따라서, 마칭 큐브 알고리즘으로 surface reconstruction을 진행한다. (…TLDR)  About Marching Cube Algorithm, tistory=&gt; 마칭 큐브 알고리즘 다음 게시물에 정리 이어서,, 느낀점  Marching Cube 알고리즘부터 완벽하게 이해를 해보자          항상 간단한 정리글로만 읽고 넘어가니까 그냥 point cloud넣고 mesh뽑아주는 그래픽스 알고리즘이다정도라고만 알고 넘어가서 모호하다, 알짜배기를 모르는 느낌      SuGaR에서는 Poisson reconstruction기반의 mesh recon알고리즘을 썼다고 되어있었는데, 이게 나는 마칭 큐브랑 완전 다른 건 줄 알았는데 또 읽다보니 포아송재건도 마칭큐브기반이라는 소리도 있고 출처가 정확하지 않으니까 혼동된다, 그래서 Poisson Reconstruction 논문도 읽어야겠다        eccv논문인데 생각보다 노벨티가 뭐가 없다          그냥 stereo-matching 모델 써서 depth추출하고 이거 기반으로 point cloud를 더 정확하고 밀도있게 뽑아내고 그 후로는 그냥 알고리즘 사용해서 메쉬추출한건데,, 성능이 좋았다는게 신기하다.      대신 단점이 명확하다, stereo 기법이다보니 위의 triangulation사진을 보다시피 베이스라인 길이에 한정된 씬만 사용될 것이므로 넓은 즉 큰 반경의 scene에 대한 mesh recon은 잘 안될 것이다 (실제로 사용한 데이터셋들도 다 작은 object based 벤치마크들이다)      3DGS도 그렇고, mesh recon도 그렇고 고질적인 문제가 투명한 transparent한 물체가 잘 복원이 어렵다는 점인데, 이부분의 개선은 왜 안되고 있는지 렌더링 측면에서 공부를 좀 더 해봐야겠다.      "
  },
  
  {
    "title": "블로그 오픈",
    "url": "/posts/First_Blog/",
    "categories": "Blogging, Tutorial",
    "tags": "personal",
    "date": "2025-01-18 02:34:00 +0900",
    





    
    "snippet": "First Blog안녕~! 데스크탑 운영체제가 리눅스여서 도커 연동이 생각만큼 잘 되지 않아 깃헙 블로그를 만드는 걸 계속 미뤄왔었다. 오늘 맥북이 새로 와서 심심해가지고 블로그 미뤄뒀던 걸 다시 도전해봤다. 아직 favicon은 반영은 뭐땜시 아직 안되고 있고,, 그대로 테마 갖다 쓰는건데도 생각보다 뭐가 잘 안되서..생각보다 오래걸렸다. 글 쓰고 ...",
    "content": "First Blog안녕~! 데스크탑 운영체제가 리눅스여서 도커 연동이 생각만큼 잘 되지 않아 깃헙 블로그를 만드는 걸 계속 미뤄왔었다. 오늘 맥북이 새로 와서 심심해가지고 블로그 미뤄뒀던 걸 다시 도전해봤다. 아직 favicon은 반영은 뭐땜시 아직 안되고 있고,, 그대로 테마 갖다 쓰는건데도 생각보다 뭐가 잘 안되서..생각보다 오래걸렸다. 글 쓰고 올리는 건 자동화가 잘돼있어서 괜찮을 것 같아 꾸준히 스터디 겸 근황을 이곳에 올리도록 하겠다.맥도 아직 익숙하지 않아서 다소 헤맸지만 좀 더 익숙해지면 활용도가 매우 높을 것 같아 만족한다!  WELCOME MY GITHUB BLOG, I am a Master student in Computer Vision Lab, Korea UniversityNext time you visit our site, this place will be much more developed and awesome."
  }
  
]

